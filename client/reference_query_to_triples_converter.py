#!/usr/bin/env python3
"""
QueryToTriplesConverter í´ë˜ìŠ¤
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì…ë ¥ë°›ì•„ triples í˜•íƒœë¡œ ë³€í™˜í•˜ê³  ê²€ìƒ‰ê¹Œì§€ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤
"""

import os
import ast
import torch
import heapq
import json
from pathlib import Path
from typing import List, Optional, Tuple, Any
from openai import OpenAI
from dotenv import load_dotenv
from tqdm import tqdm
import torch.nn.functional as F
from sentence_transformers import SentenceTransformer

load_dotenv()

# ê²€ìƒ‰ ê´€ë ¨ ìƒìˆ˜
DATASET = "drama_media_data"  # "dummy" or "media_data" or "drama_media_data"
JSON_ROOT = Path(f"output/{DATASET}/scene_graph_class/gpt-4o")
Z_CACHE = Path(f"cache/cached_graphs_{DATASET}_embed_fixed_z_ver1+2")
BERT_NAME = "sentence-transformers/all-MiniLM-L6-v2"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TOP_K = 5


class QueryToTriplesConverter:
    """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì…ë ¥ë°›ì•„ triples í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, 
                 qa_template_path: str = "templates/qa_to_triple_template.txt",
                 api_key: Optional[str] = None,
                 model: str = "gpt-4o-mini",
                 temperature: float = 0.0,
                 max_tokens: int = 256):
        """
        QueryToTriplesConverter ì´ˆê¸°í™”
        
        Args:
            qa_template_path (str): QA to triple í…œí”Œë¦¿ íŒŒì¼ ê²½ë¡œ
            api_key (str, optional): OpenAI API í‚¤. Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ
            model (str): ì‚¬ìš©í•  OpenAI ëª¨ë¸ëª…
            temperature (float): ìƒì„± ì˜¨ë„ (0.0 = ê²°ì •ì )
            max_tokens (int): ìµœëŒ€ í† í° ìˆ˜
        """
        self.qa_template_path = qa_template_path
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        # API í‚¤ ì„¤ì •
        if api_key is None:
            api_key = os.getenv("OPEN_AI_API_KEY")
        if not api_key:
            raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ OPEN_AI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ api_key íŒŒë¼ë¯¸í„°ë¥¼ ì „ë‹¬í•˜ì„¸ìš”.")
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = OpenAI(api_key=api_key)
        
        # instruction í…œí”Œë¦¿ ë¡œë“œ
        self.qa_template = self._load_qa_template()
        
        # SBERT ëª¨ë¸ ì´ˆê¸°í™”
        self.sbert = SentenceTransformer(BERT_NAME, device=DEVICE).eval()
    
    def _load_qa_template(self) -> str:
        """
        QA to triple instruction í…œí”Œë¦¿ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Returns:
            str: QA í…œí”Œë¦¿ ë‚´ìš©
        """
        try:
            # í…œí”Œë¦¿ íŒŒì¼ ì§ì ‘ ë¡œë“œ
            qa_template = Path(self.qa_template_path).read_text(encoding='utf-8')
            return qa_template
            
        except FileNotFoundError as e:
            raise FileNotFoundError(f"í…œí”Œë¦¿ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        except Exception as e:
            raise Exception(f"í…œí”Œë¦¿ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    @torch.no_grad()
    def _vec(self, txt: str) -> torch.Tensor:
        """
        í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            txt (str): ë³€í™˜í•  í…ìŠ¤íŠ¸
            
        Returns:
            torch.Tensor: ë³€í™˜ëœ ë²¡í„°
        """
        return self.sbert.encode(txt, normalize_embeddings=True, convert_to_tensor=True).float()
    
    def _token_to_sentence(self, tok: str | None) -> str:
        """
        í† í°ì„ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            tok (str | None): ë³€í™˜í•  í† í°
            
        Returns:
            str: ë³€í™˜ëœ ë¬¸ì¥
        """
        if not tok:
            return ""
        sup, typ = tok.split(":", 1) if ":" in tok else (tok, tok)
        return f"A {typ} which is a kind of {sup}."
    
    def _embed_query(self, tokens: List[str]) -> Tuple[torch.Tensor|None, ...]:
        """
        ì¿¼ë¦¬ í† í°ì„ ì„ë² ë”©í•©ë‹ˆë‹¤.
        
        Args:
            tokens (List[str]): ì„ë² ë”©í•  í† í° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Tuple[torch.Tensor|None, ...]: ì„ë² ë”©ëœ ë²¡í„°ë“¤
        """
        s_tok, v_tok, o_tok = (tokens + [None, None])[:3]
        q_s = self._vec(self._token_to_sentence(s_tok)) if s_tok else None
        q_v = self._vec(v_tok) if v_tok else None
        q_o = (
            self._vec(self._token_to_sentence(o_tok))
            if o_tok not in (None, "", "none", "None") else None
        )
        return q_s, q_v, q_o
    
    def _extract_list(self, txt: str) -> List:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            txt (str): ì¶”ì¶œí•  í…ìŠ¤íŠ¸
            
        Returns:
            List: ì¶”ì¶œëœ ë¦¬ìŠ¤íŠ¸
        """
        start, end = txt.find("["), txt.rfind("]")
        if start == -1 or end == -1 or end <= start:
            return []
        try:
            # nullì„ Noneìœ¼ë¡œ ë³€í™˜
            list_str = txt[start : end + 1].replace('null', 'None')
            return ast.literal_eval(list_str)
        except Exception as e:
            print(f"âŒ ë¦¬ìŠ¤íŠ¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"   ì›ë³¸ í…ìŠ¤íŠ¸: {txt[start : end + 1]}")
            return []
    
    def _triples_in_scene(self, js) -> List[Tuple[int, int, Optional[int], str]]:
        """
        ì¥ë©´ ê·¸ë˜í”„ì—ì„œ triplesë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            js: ì¥ë©´ ê·¸ë˜í”„ JSON ê°ì²´
            
        Returns:
            List[Tuple[int, int, Optional[int], str]]: ì¶”ì¶œëœ triples
        """
        out, g = [], js["scene_graph"]
        for ev in g.get("events", []):
            s, o = ev.get("subject"), ev.get("object")
            if s is None:
                continue
            # objectê°€ ì •ìˆ˜ IDê°€ ì•„ë‹ ê²½ìš° None ì²˜ë¦¬
            o = o if isinstance(o, int) else None
            out.append((s, ev["event_id"], o, ev.get("verb", "")))
        return out
    
    def _search_topk_multi(self, queries_emb: List[Tuple], tau: float, k: int = TOP_K):
        """
        ì—¬ëŸ¬ ì¿¼ë¦¬ì— ëŒ€í•´ top-k ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            queries_emb (List[Tuple]): ì„ë² ë”©ëœ ì¿¼ë¦¬ë“¤
            tau (float): ìœ ì‚¬ë„ ì„ê³„ê°’
            k (int): ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            List: ê²€ìƒ‰ ê²°ê³¼
        """
        heap = []
        total_q = len(queries_emb)

        for pt_fp in tqdm(list(Z_CACHE.rglob("*.pt")), desc="search"):
            blob = torch.load(pt_fp, map_location="cpu")
            z = F.normalize(blob["z"], dim=1)
            id2idx = {nid: i for i, nid in enumerate(blob["orig_id"])}

            rel_path = Path(blob["path"])
            js_fp = JSON_ROOT / rel_path
            if not js_fp.exists(): 
                continue
            scene_triples = self._triples_in_scene(json.load(js_fp.open()))
            if not scene_triples: 
                continue

            matched = []
            used = set()

            for q_idx, (q_s, q_v, q_o) in enumerate(queries_emb):
                best = None
                for sid, eid, oid, _ in scene_triples:
                    # tripleì˜ ê° ìš”ì†Œê°€ listì¸ ê²½ìš° ìŠ¤í‚µ
                    if any(isinstance(x, list) for x in (sid, eid, oid)):
                        continue
                    if sid not in id2idx or eid not in id2idx: 
                        continue
                    # ê°ì²´ í•„ìˆ˜ ì—¬ë¶€ íŒë‹¨
                    need_obj = q_o is not None
                    if need_obj and oid is None:              
                        continue

                    v_s = z[id2idx[sid]]
                    v_v = z[id2idx[eid]]
                    v_o = z[id2idx[oid]] if (oid is not None and oid in id2idx) else None

                    # ê°ì²´ ìœ íš¨ì„± ì ê²€
                    if need_obj and v_o is None: 
                        continue

                    s_sim = float(torch.dot(q_s, v_s)) if q_s is not None else None
                    v_sim = float(torch.dot(q_v, v_v)) if q_v is not None else None
                    o_sim = (
                        float(torch.dot(q_o, v_o))
                        if (q_o is not None and v_o is not None) else None
                    )

                    # ì„ê³„ì¹˜ ê²€ì‚¬
                    if (q_s is not None and s_sim < tau) or \
                       (q_v is not None and v_sim < tau) or \
                       (q_o is not None and o_sim < tau):
                        continue

                    sims = [x for x in (s_sim, v_sim, o_sim) if x is not None]
                    sim = sum(sims) / len(sims)

                    if best is None or sim > best[0]:
                        best = (sim, s_sim, v_sim, o_sim, (sid, eid, oid))
                if best:
                    matched.append((q_idx,) + best)
                    used.add(best[-1])

            if not matched: 
                continue
            match_cnt = len(matched)
            avg_sim = sum(m[1] for m in matched) / match_cnt
            heapq.heappush(heap, (match_cnt, avg_sim, matched, rel_path.parts[0], rel_path, total_q))
            if len(heap) > k: 
                heapq.heappop(heap)

        return sorted(heap, key=lambda x: (-x[0], -x[1]))
    
    def __call__(self, question: str, tau: float = 0.30, top_k: int = TOP_K) -> dict:
        """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì…ë ¥ë°›ì•„ triplesë¡œ ë³€í™˜í•˜ê³  ê²€ìƒ‰ê¹Œì§€ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            question (str): ì‚¬ìš©ìì˜ ì§ˆë¬¸
            tau (float): ìœ ì‚¬ë„ ì„ê³„ê°’
            top_k (int): ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            dict: ë³€í™˜ëœ triplesì™€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # 1. ì§ˆë¬¸ì„ triplesë¡œ ë³€í™˜
            print(f"ğŸš€ ì§ˆë¬¸ì„ triplesë¡œ ë³€í™˜ ì¤‘...")
            print(f"ì§ˆë¬¸: {question}")
            
            # instruction í…œí”Œë¦¿ì— ì§ˆë¬¸ ì‚½ì…
            prompt = self.qa_template.replace("$CONTENT", question)
            
            # OpenAI API í˜¸ì¶œ
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
            )
            
            # ì‘ë‹µ ë‚´ìš© ì¶”ì¶œ
            content = response.choices[0].message.content
            print(f"LLM ì‘ë‹µ:\n{content}")
            
            # triples ì¶”ì¶œ
            triples = self._extract_list(content)
            
            # triples í˜•íƒœ ì •ê·œí™” (ë‹¨ì¼ tripleì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ê¸°)
            if triples and not isinstance(triples[0], list):
                triples = [triples]
            
            print(f"âœ… ë³€í™˜ ì™„ë£Œ: {len(triples)}ê°œ triple ìƒì„±")
            
            if not triples:
                print("âŒ triplesê°€ ìƒì„±ë˜ì§€ ì•Šì•„ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return {
                    "question": question,
                    "triples": [],
                    "search_results": [],
                    "success": False
                }
            
            # 2. triples ì„ë² ë”©
            print(f"ğŸš€ triples ì„ë² ë”© ì¤‘...")
            queries_emb = [self._embed_query(t) for t in triples]
            
            # 3. ê²€ìƒ‰ ìˆ˜í–‰
            print(f"ğŸš€ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘... (tau={tau}, top_k={top_k})")
            search_results = self._search_topk_multi(queries_emb, tau, top_k)
            
            print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            
            # 4. ê²°ê³¼ ë°˜í™˜
            return {
                "question": question,
                "triples": triples,
                "search_results": search_results,
                "success": True,
                "tau": tau,
                "top_k": top_k
            }
            
        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "question": question,
                "triples": [],
                "search_results": [],
                "success": False,
                "error": str(e)
            }
    
    def convert_question(self, question: str) -> List[List[str]]:
        """
        ì§ˆë¬¸ì„ triplesë¡œë§Œ ë³€í™˜í•©ë‹ˆë‹¤ (ê²€ìƒ‰ ì—†ìŒ).
        
        Args:
            question (str): ì‚¬ìš©ìì˜ ì§ˆë¬¸
            
        Returns:
            List[List[str]]: ë³€í™˜ëœ triples ë¦¬ìŠ¤íŠ¸
        """
        try:
            # instruction í…œí”Œë¦¿ì— ì§ˆë¬¸ ì‚½ì…
            prompt = self.qa_template.replace("$CONTENT", question)
            
            # OpenAI API í˜¸ì¶œ
            print(f"ğŸš€ ì§ˆë¬¸ì„ triplesë¡œ ë³€í™˜ ì¤‘...")
            print(f"ì§ˆë¬¸: {question}")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
            )
            
            # ì‘ë‹µ ë‚´ìš© ì¶”ì¶œ
            content = response.choices[0].message.content
            print(f"LLM ì‘ë‹µ:\n{content}")
            
            # triples ì¶”ì¶œ
            triples = self._extract_list(content)
            
            # triples í˜•íƒœ ì •ê·œí™” (ë‹¨ì¼ tripleì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ê¸°)
            if triples and not isinstance(triples[0], list):
                triples = [triples]
            
            print(f"âœ… ë³€í™˜ ì™„ë£Œ: {len(triples)}ê°œ triple ìƒì„±")
            return triples
            
        except Exception as e:
            print(f"âŒ ì§ˆë¬¸ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    def get_template_info(self) -> dict:
        """
        í˜„ì¬ ë¡œë“œëœ í…œí”Œë¦¿ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            dict: í…œí”Œë¦¿ ì •ë³´
        """
        return {
            "qa_template_path": self.qa_template_path,
            "model": self.model,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "template_length": len(self.qa_template)
        }
    
    def update_model(self, model: str):
        """
        ì‚¬ìš©í•  ëª¨ë¸ì„ ë³€ê²½í•©ë‹ˆë‹¤.
        
        Args:
            model (str): ìƒˆë¡œìš´ ëª¨ë¸ëª…
        """
        self.model = model
        print(f"ëª¨ë¸ì´ {model}ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def update_parameters(self, temperature: Optional[float] = None, max_tokens: Optional[int] = None):
        """
        ìƒì„± íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            temperature (float, optional): ìƒˆë¡œìš´ ì˜¨ë„ ê°’
            max_tokens (int, optional): ìƒˆë¡œìš´ ìµœëŒ€ í† í° ìˆ˜
        """
        if temperature is not None:
            self.temperature = temperature
        if max_tokens is not None:
            self.max_tokens = max_tokens
        print(f"íŒŒë¼ë¯¸í„°ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤: temperature={self.temperature}, max_tokens={self.max_tokens}")

