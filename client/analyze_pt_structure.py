#!/usr/bin/env python3
"""
PT íŒŒì¼ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ë…¸ë“œ íƒ€ì…ë³„ ì„ë² ë”© ì •ë³´ê°€ ì–´ë–»ê²Œ êµ¬ë¶„ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
"""

import os
import torch
import numpy as np
from pathlib import Path

def analyze_pt_file(pt_file_path: str):
    """PT íŒŒì¼ì˜ êµ¬ì¡°ë¥¼ ìƒì„¸íˆ ë¶„ì„"""
    print(f"ğŸ” PT íŒŒì¼ ë¶„ì„: {os.path.basename(pt_file_path)}")
    print("=" * 60)
    
    try:
        # PT íŒŒì¼ ë¡œë“œ
        pt_data = torch.load(pt_file_path, map_location='cpu')
        
        print(f"ğŸ“Š PT íŒŒì¼ í‚¤ë“¤: {list(pt_data.keys())}")
        print()
        
        # ê° í‚¤ë³„ ìƒì„¸ ë¶„ì„
        for key, value in pt_data.items():
            print(f"ğŸ”‘ í‚¤: {key}")
            if hasattr(value, 'shape'):
                print(f"   - íƒ€ì…: {type(value)}")
                print(f"   - í˜•íƒœ: {value.shape}")
                if key == 'z':
                    print(f"   - ì„ë² ë”© ë²¡í„° (ì²« 3ê°œ):")
                    for i in range(min(3, value.shape[0])):
                        print(f"     [{i}]: {value[i][:5].tolist()}... (384ì°¨ì›)")
                elif key == 'node_type':
                    print(f"   - ë…¸ë“œ íƒ€ì… ê°’ë“¤: {value.tolist()}")
                    unique_types = torch.unique(value)
                    print(f"   - ê³ ìœ  ë…¸ë“œ íƒ€ì…: {unique_types.tolist()}")
                    for node_type in unique_types:
                        count = (value == node_type).sum().item()
                        print(f"     - íƒ€ì… {node_type}: {count}ê°œ")
                elif key == 'orig_id':
                    print(f"   - orig_id ê°’ë“¤: {value.tolist()}")
                    print(f"   - orig_id ë²”ìœ„: {value.min().item()} ~ {value.max().item()}")
                    print(f"   - ê³ ìœ  orig_id: {len(torch.unique(value))}ê°œ")
            else:
                print(f"   - íƒ€ì…: {type(value)}")
                print(f"   - ê°’: {value}")
            print()
        
        # ë…¸ë“œ íƒ€ì…ë³„ orig_id ë¶„ì„
        if 'node_type' in pt_data and 'orig_id' in pt_data:
            print("ğŸ“‹ ë…¸ë“œ íƒ€ì…ë³„ orig_id ë¶„ì„:")
            print("-" * 40)
            
            node_types = pt_data['node_type']
            orig_ids = pt_data['orig_id']
            
            for node_type in torch.unique(node_types):
                mask = node_types == node_type
                type_orig_ids = orig_ids[mask]
                
                print(f"ğŸ”¸ ë…¸ë“œ íƒ€ì… {node_type}:")
                print(f"   - ê°œìˆ˜: {len(type_orig_ids)}")
                print(f"   - orig_id ë²”ìœ„: {type_orig_ids.min().item()} ~ {type_orig_ids.max().item()}")
                print(f"   - orig_id ê°’ë“¤: {type_orig_ids.tolist()}")
                print()
        
        return True
        
    except Exception as e:
        print(f"âŒ PT íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ” PT íŒŒì¼ êµ¬ì¡° ë¶„ì„")
    print("=" * 60)
    
    data2_path = Path("data2")
    if not data2_path.exists():
        print("âŒ data2 í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # PT íŒŒì¼ë“¤ ì°¾ê¸°
    pt_files = list(data2_path.glob("*.pt"))
    if not pt_files:
        print("âŒ PT íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ë°œê²¬ëœ PT íŒŒì¼: {len(pt_files)}ê°œ")
    print()
    
    # ê° PT íŒŒì¼ ë¶„ì„
    for i, pt_file in enumerate(pt_files, 1):
        print(f"ğŸ“„ [{i}/{len(pt_files)}] {pt_file.name}")
        analyze_pt_file(str(pt_file))
        print("\n" + "=" * 60 + "\n")

if __name__ == "__main__":
    main()
